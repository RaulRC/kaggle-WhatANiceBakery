{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e926da86595bd367a5ff288136bd3126c30f848"},"cell_type":"markdown","source":"## **Read file**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/BreadBasket_DMS.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7750f49a57fd1252bd24a5dc9c54cf5457e5902c"},"cell_type":"markdown","source":"## Exploatory analysis"},{"metadata":{"trusted":true,"_uuid":"34dd14b75961f38e4334301963f6a31fd626ebad"},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c189dce7d3c4ac58b357bd5417f4283828f2ee9"},"cell_type":"markdown","source":"**RRC observation**: Huh, interesting. A dataframe line is a transaction or a part of it. So, same transaction number implies is part of the same payment. See line 0 (Transaction=1) vs lines 1 and 2 (Transaction=2)"},{"metadata":{"trusted":true,"_uuid":"5a4712839a428182ce997c118c4a35e465e1624c"},"cell_type":"code","source":"print(\"Length of the dataset: %d\\nNumber of different items: %d\" % (len(df), len(df.Item.unique())))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0492ad88a9e252fba5576e2b1ac977a0efd1954f"},"cell_type":"code","source":"print(\"Number of different transactions: %d\" % df['Transaction'].max())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"805ee631ad933f4043e424cb73a1886164ec2dfe"},"cell_type":"markdown","source":"**RRC observation**: Ok, almost 10k different transaction over a total of 21k lines of dataset. Expecting ~2 items per transaction as average, right? Let's confirm. "},{"metadata":{"trusted":true,"_uuid":"018b916549b40699b6e0603c7e349f18912d420a"},"cell_type":"code","source":"nTransactions = df.groupby(['Transaction'])['Item'].count().reset_index()\nnTransactions.columns = ['Transaction', 'nItems']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad86b72af1ae24232193927cd040745aab7eb11c"},"cell_type":"code","source":"#Checking...\nnTransactions.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6c5ab048a0fdfebd12b3ac6cab0002c4e56dbfe"},"cell_type":"code","source":"print(\"Number of Items per transaction in average: %.2f\" % nTransactions['nItems'].mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fafec4f42c2fed6ef95142d12177543561bb2f2a"},"cell_type":"markdown","source":"**RRC**: Done! Ok, but let's dig a bit more... "},{"metadata":{"trusted":true,"_uuid":"8e581745672bb3f473da8aa2174a9b57fd0666da"},"cell_type":"code","source":"nTransactions['nItems'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26e4e532bf52ead893c61bdf4a7e9d48da25128b"},"cell_type":"markdown","source":"**RRC**: Whoa! 12 is the max. And nice quartiles as well... Such a great information to work with.  "},{"metadata":{"_uuid":"2855f1b3e8f22d944864e2a7dc9de7656f84e716"},"cell_type":"markdown","source":"\n**RRC**: Ok, first ideas.\n* Let's see the distribution of the transactions along the time. First, hour of the day. Then we will calculate the day of the with corresponding to the date and see what do we have. \n* Evaluate which items are most demanded. \n* Evaluate those Items in time. "},{"metadata":{"trusted":true,"_uuid":"6350376738f6cc3afe34548b234abded73c08498"},"cell_type":"code","source":"#Transforming original dataset. First, adding the new column \"number of Items, nItems\" to the original. Just joining by Transaction .\n# tt stands for \"transformed Transactions\" :o)\n#On the other hand, we build 'times' dataset for time analysis\n\ntt = nTransactions.merge(df, on='Transaction')\ntimes = tt.drop_duplicates(subset='Transaction', keep='first')[['Transaction', 'nItems', 'Date', 'Time']]\ntimes.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37090867b145bf833c30b8b852574d710b016dfc"},"cell_type":"code","source":"timeCount = times.groupby('Time').count()['Date'].reset_index()\ntimeCount.head()\n#Ok, let's group by a more generic hour. What about hour and decimal of minutes? ;)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12cd67250c8f707e9e7217ea078734baaecbe5ff"},"cell_type":"code","source":"MIN_UNIT_INDEX=4\nHOUR_UNIT_INDEX=2\ntimes['hourMin'] = times['Time'].apply(lambda x: str(x)[:HOUR_UNIT_INDEX]+\"x\")\ntimes.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dca3f9f7813c3399fb9167163cfcbd7b45eb6b28"},"cell_type":"code","source":"timeCount = times.groupby('hourMin').count()['Date'].reset_index()\ntimeCount.columns=['hourMin', 'countTransactions']\n\ntimeCount.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fafb20f1e45c86829a1390a8eb5a84d51a521520"},"cell_type":"code","source":"len(timeCount)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a094a2a1b28c6366e87cf7c49343953feba1b27"},"cell_type":"code","source":"timeCount.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"894fad14420c689fab08ada1947e8d615329c294"},"cell_type":"code","source":"timeCount.plot.bar(x='hourMin', y='countTransactions', rot=30, figsize=(15,10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20584dfaddcd04112ac656186fc892143463981e"},"cell_type":"markdown","source":"**RRC**: Now, same analysis by day of the week. "},{"metadata":{"trusted":true,"_uuid":"5b4d5c2cb7515a079245bb44097b7adf13927a93"},"cell_type":"code","source":"times['dayWeek'] = pd.to_datetime(times['Date']).dt.weekday_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4041b519d3204613811105f53cf3bcdaa5baa4cf"},"cell_type":"code","source":"times['dayWeek'] = times['dayWeek'].dt.dayofweek","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14eb1a3dc9b83196160522ea7caa60e7069cb93d"},"cell_type":"code","source":"times.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7836a09f7e48192e7991ff44e23d0a4071e8e558"},"cell_type":"code","source":"timesDay = times.groupby('dayWeek').count()['Transaction'].reset_index()\ntimesDay.columns = ['dayWeek', 'nTransactions']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55933217a104992a62ed912294d40266dd8df474"},"cell_type":"code","source":"timesDay.plot.bar(x='dayWeek', y='nTransactions', rot=30, figsize=(15,10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"daf0edca4c3e4df5785c4d58029372a054d624d0"},"cell_type":"markdown","source":"**RRC**: Now, let's join this dataframe with the original in order to get this info at line level"},{"metadata":{"trusted":true,"_uuid":"730278c5394853b3626c9014187677710884049c"},"cell_type":"code","source":"times = times[['Transaction', 'nItems', 'hourMin', 'dayWeek']]\ndfplus = df.merge(times, on='Transaction')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f25baddb190eb3563a8832749f167b160cb2fec"},"cell_type":"code","source":"dfplus.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f6fef8e6b36668ba866ee958aa339c4e6c55855"},"cell_type":"code","source":"res = dfplus.groupby(['dayWeek', 'Item']).count()['nItems'].reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8ba04b8f6a0f20d0021173e6276dd45d9bb1503"},"cell_type":"code","source":"res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc3d0508e308b634fae78db6e90ffb7178181398"},"cell_type":"code","source":"mostPerDay = res.groupby('dayWeek').agg(['min', 'max'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f1d1b337fd88194ebfa462652c95ced752abedd"},"cell_type":"code","source":"mostPerDay","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba7b05a3e75adfb7594abc8dd0c9d8a1e75470c2"},"cell_type":"markdown","source":"**RRC**: Cool! Let's calculate top of the items selled"},{"metadata":{"trusted":true,"_uuid":"46e2f1e986407c343d5fe22a3cb1d8f06a54f5c3"},"cell_type":"code","source":"totalItems = res.groupby('Item').sum().sort_values(by='nItems', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d34b63362afadd3eff9a0a86104653e5a68a0d17"},"cell_type":"code","source":"# Top 10 ;P\ntotalItems[:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8798f98091ecb8b883c00c3bebb902d2f27e1a7d"},"cell_type":"markdown","source":"## Some conclusions\n\n* The top Item is Coffee (of course!)\n* But the max items selled per day are the Vegan mincepie and the Victorian Sponge\n* Would be nice to see how the sellings of this items evolve on time. "},{"metadata":{"trusted":true,"_uuid":"b7d2bae543354d87d194fab5d2c5a51d55e26671"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}